{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ae310e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:52:16.860199Z",
     "iopub.status.busy": "2025-04-21T04:52:16.859931Z",
     "iopub.status.idle": "2025-04-21T04:53:46.132228Z",
     "shell.execute_reply": "2025-04-21T04:53:46.130864Z"
    },
    "id": "SK-iPJ4IRNBg",
    "outputId": "487b8ad5-8678-437b-c5c0-dc78c62934a4",
    "papermill": {
     "duration": 89.278174,
     "end_time": "2025-04-21T04:53:46.134253",
     "exception": false,
     "start_time": "2025-04-21T04:52:16.856079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\r\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting notion_client\r\n",
      "  Downloading notion_client-2.3.0-py2.py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting yt_dlp\r\n",
      "  Downloading yt_dlp-2025.3.31-py3-none-any.whl.metadata (172 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.11/dist-packages (0.3.6)\r\n",
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\r\n",
      "Collecting faiss-cpu\r\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\r\n",
      "Collecting genanki\r\n",
      "  Downloading genanki-0.13.1-py3-none-any.whl.metadata (7.5 kB)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.5.1+cu124)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\r\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\r\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\r\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\r\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from notion_client) (0.28.1)\r\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_text_splitters) (0.3.35)\r\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\r\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\r\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\r\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\r\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\r\n",
      "Collecting cached-property (from genanki)\r\n",
      "  Downloading cached_property-2.0.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from genanki) (2.4.6)\r\n",
      "Collecting chevron (from genanki)\r\n",
      "  Downloading chevron-0.14.0-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from genanki) (6.0.2)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.67.0)\r\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion_client) (3.7.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion_client) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion_client) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->notion_client) (3.10)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->notion_client) (0.14.0)\r\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.3.8)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (9.0.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (1.33)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.18.0)\r\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\r\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\r\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\r\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\r\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (3.0.0)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (3.10.15)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (1.0.0)\r\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.23.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->notion_client) (1.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->openai-whisper) (2024.2.0)\r\n",
      "Downloading notion_client-2.3.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Downloading yt_dlp-2025.3.31-py3-none-any.whl (3.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading genanki-0.13.1-py3-none-any.whl (16 kB)\r\n",
      "Downloading cached_property-2.0.1-py3-none-any.whl (7.4 kB)\r\n",
      "Downloading chevron-0.14.0-py3-none-any.whl (11 kB)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\r\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803407 sha256=fe964c42ff3110f956687caf9407ea8b12a0aea38c0525c08f10e9101258b6cc\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\r\n",
      "Successfully built openai-whisper\r\n",
      "Installing collected packages: chevron, yt_dlp, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, cached-property, nvidia-cusparse-cu12, nvidia-cudnn-cu12, genanki, nvidia-cusolver-cu12, notion_client, openai-whisper, faiss-cpu\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cached-property-2.0.1 chevron-0.14.0 faiss-cpu-1.10.0 genanki-0.13.1 notion_client-2.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 yt_dlp-2025.3.31\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper notion_client yt_dlp langchain_text_splitters google-generativeai faiss-cpu genanki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3b4935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:46.182735Z",
     "iopub.status.busy": "2025-04-21T04:53:46.182404Z",
     "iopub.status.idle": "2025-04-21T04:53:53.917362Z",
     "shell.execute_reply": "2025-04-21T04:53:53.916596Z"
    },
    "id": "PwUVTPBHRNBj",
    "papermill": {
     "duration": 7.760435,
     "end_time": "2025-04-21T04:53:53.918780",
     "exception": false,
     "start_time": "2025-04-21T04:53:46.158345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import whisper\n",
    "from notion_client import Client\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import configure, GenerativeModel\n",
    "from notion_client import Client\n",
    "from google.colab import userdata\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import genanki\n",
    "import yt_dlp\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2b78f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:53.964762Z",
     "iopub.status.busy": "2025-04-21T04:53:53.964068Z",
     "iopub.status.idle": "2025-04-21T04:53:53.968374Z",
     "shell.execute_reply": "2025-04-21T04:53:53.967679Z"
    },
    "id": "jci1BbZmRNBl",
    "papermill": {
     "duration": 0.028434,
     "end_time": "2025-04-21T04:53:53.969483",
     "exception": false,
     "start_time": "2025-04-21T04:53:53.941049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model definition \n",
    "\n",
    "model = GenerativeModel('gemini-2.0-flash-thinking-exp-01-21',generation_config={\"temperature\": 0})\n",
    "embedding_model = 'models/text-embedding-004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd1fb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.014927Z",
     "iopub.status.busy": "2025-04-21T04:53:54.014671Z",
     "iopub.status.idle": "2025-04-21T04:53:54.019559Z",
     "shell.execute_reply": "2025-04-21T04:53:54.018919Z"
    },
    "id": "6f00X4JMRNBm",
    "papermill": {
     "duration": 0.02927,
     "end_time": "2025-04-21T04:53:54.020887",
     "exception": false,
     "start_time": "2025-04-21T04:53:53.991617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_youtube_audio(url, filename=\"audio.mp3\"):\n",
    "    ydl_opts = {\n",
    "        'format':'bestaudio/best',\n",
    "        'outtmpl':'audio.mp3',\n",
    "        'noplaylist':True,\n",
    "        'extract_audio':True,\n",
    "        'audioformat':'mp3',\n",
    "        'audioquality':192,\n",
    "        'overwrites': True\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return f\"{filename}\"\n",
    "\n",
    "# transcribe the audio\n",
    "def transcribe_audio(audio_file):\n",
    "    audio_file = \"/kaggle/working/\"+audio_file\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_file)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220d7d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.066360Z",
     "iopub.status.busy": "2025-04-21T04:53:54.066052Z",
     "iopub.status.idle": "2025-04-21T04:53:54.074264Z",
     "shell.execute_reply": "2025-04-21T04:53:54.073510Z"
    },
    "id": "k13t2vVkRNBm",
    "papermill": {
     "duration": 0.031653,
     "end_time": "2025-04-21T04:53:54.075211",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.043558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_embeddings_and_index(transcript):\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(transcript)\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"Error: No chunks created from the transcript.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = []\n",
    "    try:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"  Embedding chunk {i+1}/{len(chunks)}\")\n",
    "# Use 'RETRIEVAL_DOCUMENT' for texts being indexed\n",
    "            result = genai.embed_content(model=embedding_model,\n",
    "                                         content=chunk,\n",
    "                                         task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "            embeddings.append(result['embedding'])\n",
    "\n",
    "        if not embeddings:\n",
    "            print(\"Error: No embeddings were generated.\")\n",
    "            return None, None, None\n",
    "# FAISS requires float32\n",
    "        dimension = len(embeddings[0])\n",
    "        embeddings_np = np.array(embeddings).astype('float32')\n",
    "\n",
    "        print(f\"Creating FAISS index with dimension {dimension}...\")\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings_np)\n",
    "        print(f\"FAISS index created successfully with {index.ntotal} vectors.\")\n",
    "        return index, chunks, dimension\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during embedding or indexing: {e}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff4c625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.209546Z",
     "iopub.status.busy": "2025-04-21T04:53:54.209230Z",
     "iopub.status.idle": "2025-04-21T04:53:54.214913Z",
     "shell.execute_reply": "2025-04-21T04:53:54.214284Z"
    },
    "id": "xdkj-4VERNBn",
    "papermill": {
     "duration": 0.030026,
     "end_time": "2025-04-21T04:53:54.215974",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.185948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_key_terms(transcript,num_flashcards,user_prompts):\n",
    "    print(\"Extracting key terms...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in identifying the most important concepts and terminology from educational content.\n",
    "    Analyze the following transcript and extract MAXIMUM {num_flashcards} key terms or phrases that would be suitable for creating flashcards.\n",
    "    Focus on {user_prompts}. But ONLY MAXIMUM {num_flashcards} MOST IMPORTANT!\n",
    "\n",
    "    Return the results as a JSON list under the key \"terms\".\n",
    "    Also give me a heading for the transcript. Return this under the key \"heading\".\n",
    "\n",
    "    Example JSON format: {{\"heading\":\"place the heading here\",\"terms\": [\"term1\", \"concept phrase 2\", \"important name\"]}}\n",
    "\n",
    "\n",
    "    Transcript:\n",
    "    ---\n",
    "    {transcript}\n",
    "    ---\n",
    "\n",
    "    Extract key terms and return ONLY the JSON object:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "# Robust parsing attempt\n",
    "        cleaned_response = response.text.strip().replace('```json', '').replace('```', '')\n",
    "        data = json.loads(cleaned_response)\n",
    "        key_terms = data.get(\"terms\", [])\n",
    "        heading = data.get(\"heading\",[])\n",
    "        if not isinstance(key_terms, list):\n",
    "             print(f\"Warning: 'terms' field is not a list in the response: {data}\")\n",
    "             return []\n",
    "        print(f\"Extracted {len(key_terms)} key terms.\")\n",
    "        return key_terms, heading\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response for key terms: {e}\")\n",
    "        print(f\"LLM Raw Response: {response.text}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error during key term extraction: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac12e655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.262499Z",
     "iopub.status.busy": "2025-04-21T04:53:54.261672Z",
     "iopub.status.idle": "2025-04-21T04:53:54.269782Z",
     "shell.execute_reply": "2025-04-21T04:53:54.269012Z"
    },
    "id": "dcSdv4C8RNBo",
    "papermill": {
     "duration": 0.032586,
     "end_time": "2025-04-21T04:53:54.271084",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.238498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_flashcards(key_terms, index, chunks, confidence, k_neighbors=2):\n",
    "    print(f\"Generating flashcards for {len(key_terms)} terms...\")\n",
    "    flashcards = []\n",
    "    if not index or not chunks:\n",
    "        print(\"Error: Index or chunks are missing, cannot generate flashcards.\")\n",
    "        return []\n",
    "\n",
    "    for i, term in enumerate(key_terms):\n",
    "        print(f\"  Processing term {i+1}/{len(key_terms)}: '{term}'\")\n",
    "        try:\n",
    "# 1. Get query embedding\n",
    "# Use 'RETRIEVAL_QUERY' for the search term\n",
    "            query_embedding_result = genai.embed_content(model=embedding_model,\n",
    "                                                         content=term,\n",
    "                                                         task_type=\"RETRIEVAL_QUERY\")\n",
    "            query_embedding = np.array([query_embedding_result['embedding']]).astype('float32')\n",
    "\n",
    "# 2. Search FAISS index\n",
    "            distances, indices = index.search(query_embedding, k_neighbors)\n",
    "\n",
    "# 3. Construct context from retrieved chunks\n",
    "            context_chunks = [chunks[idx] for idx in indices[0] if idx < len(chunks)] # Safety check\n",
    "            context = \"\\n---\\n\".join(context_chunks) # Separate chunks clearly\n",
    "\n",
    "            if not context:\n",
    "                 print(f\"    Warning: No context found for term '{term}'. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "# 4. Generate flashcard with context\n",
    "            prompt = f\"\"\"\n",
    "            You are an expert flashcard creator. Based *only* on the provided context,\n",
    "            create a single flashcard for the key term: '{term}'.\n",
    "\n",
    "            Follow these instructions precisely:\n",
    "            1.  Give concise and accurate information about the key term '{term}', summarizing the relevant information from the context.\n",
    "            2.  Include factual information not present in the context related to the key term '{term}'.\n",
    "            3.  Rate your confidence (from 0.0 to 1.0) in the accuracy and relevance of the information based *solely* on the provided context. 1.0 means high confidence, 0.0 means no confidence.\n",
    "            4.  Return the result as a single JSON object with the keys \"topic\", \"information\", and \"confidence\".\n",
    "            5.  Information should be not more than 100 words.\n",
    "\n",
    "            Key Term: \"{term}\"\n",
    "\n",
    "            Context:\n",
    "            ---\n",
    "            {context}\n",
    "            ---\n",
    "\n",
    "            Generate the flashcard JSON object:\n",
    "            \"\"\"\n",
    "            response = model.generate_content(prompt)\n",
    "# Robust parsing attempt\n",
    "            cleaned_response = response.text.strip().replace('```json', '').replace('```', '')\n",
    "            card = json.loads(cleaned_response)\n",
    "\n",
    "# 5. Validate and filter\n",
    "            if not all(k in card for k in [\"topic\", \"information\", \"confidence\"]):\n",
    "                 print(f\"    Warning: Generated card for '{term}' missing required keys. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            if card.get(\"confidence\", 0) >= confidence:  # Quality filter\n",
    "                flashcards.append(card)\n",
    "                print(f\"    Successfully generated flashcard for '{term}'.\")\n",
    "            else:\n",
    "                print(f\"    Skipping card for '{term}' due to low confidence ({card.get('confidence')}).\")\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"    Error decoding JSON response for term '{term}': {e}\")\n",
    "            print(f\"    LLM Raw Response: {response.text}\")\n",
    "            continue # Skip to the next term\n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing term '{term}': {e}\")\n",
    "            continue # Skip to the next term\n",
    "\n",
    "    print(f\"Generated {len(flashcards)} high-confidence flashcards.\")\n",
    "    return flashcards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6fd074c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.317519Z",
     "iopub.status.busy": "2025-04-21T04:53:54.317185Z",
     "iopub.status.idle": "2025-04-21T04:53:54.323639Z",
     "shell.execute_reply": "2025-04-21T04:53:54.322491Z"
    },
    "id": "-6Uimz12RNBq",
    "papermill": {
     "duration": 0.030647,
     "end_time": "2025-04-21T04:53:54.324831",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.294184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_notion(flashcards, notion_key, page_id,final_link,heading=\"New Flashcard Set\"):\n",
    "    notion = Client(auth=notion_key)\n",
    "\n",
    "    # 1. Create a new parent page (optional, but useful for organization)\n",
    "    parent_page = notion.pages.create(\n",
    "        parent={\"type\": \"page_id\", \"page_id\": page_id},\n",
    "        properties={\"title\": {\"title\": [{\"text\": {\"content\": heading}}]}},\n",
    "    )\n",
    "\n",
    "    # 2. Create a new database inside the parent page\n",
    "    new_database = notion.databases.create(\n",
    "        parent={\"type\": \"page_id\", \"page_id\": parent_page[\"id\"]},\n",
    "        title=[{\"type\": \"text\", \"text\": {\"content\": \"Flashcards\"}}],\n",
    "        properties={\n",
    "            \"Topic\": {\"title\": {}},  # Title column (for questions)\n",
    "            \"Information\": {\"rich_text\": {}},  # Rich text column (for answers)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 3. Insert flashcards into the new database\n",
    "    for card in flashcards:\n",
    "        print(f\"Uploading flashcard: {flashcards.index(card)+1}/{len(flashcards)}\")\n",
    "        notion.pages.create(\n",
    "            parent={\"database_id\": new_database[\"id\"]},\n",
    "            properties={\n",
    "                \"Topic\": {\n",
    "                    \"title\": [{\"text\": {\"content\": card[\"topic\"]}}]\n",
    "                },\n",
    "                \"Information\": {\n",
    "                    \"rich_text\": [{\"text\": {\"content\": card[\"information\"]}}]\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"Created a new Notion database and uploaded {len(flashcards)} cards. You can access them here: {new_database['url']}\")\n",
    "    return new_database[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd88df07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.368643Z",
     "iopub.status.busy": "2025-04-21T04:53:54.368345Z",
     "iopub.status.idle": "2025-04-21T04:53:54.373773Z",
     "shell.execute_reply": "2025-04-21T04:53:54.372998Z"
    },
    "id": "XpktUY12cxTp",
    "papermill": {
     "duration": 0.028465,
     "end_time": "2025-04-21T04:53:54.374856",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.346391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_anki_deck(flashcards, deck_name=\"YouTube Flashcards\"):\n",
    "    \"\"\"Create an Anki deck and add flashcards to it.\"\"\"\n",
    "\n",
    "    # Define a basic model for our cards\n",
    "    my_model = genanki.Model(\n",
    "        1607392319,  # Random model ID\n",
    "        'Simple Model',\n",
    "        fields=[\n",
    "            {'name': 'Question'},\n",
    "            {'name': 'Answer'},\n",
    "        ],\n",
    "        templates=[\n",
    "            {\n",
    "                'name': 'Card 1',\n",
    "                'qfmt': '{{Question}}',\n",
    "                'afmt': '{{FrontSide}}<hr id=\"answer\">{{Answer}}',\n",
    "            },\n",
    "        ])\n",
    "\n",
    "    # Create a new deck\n",
    "    my_deck = genanki.Deck(\n",
    "        2059400110,  # Random deck ID\n",
    "        deck_name)\n",
    "\n",
    "    # Add notes (flashcards) to the deck\n",
    "    for card in flashcards:\n",
    "        my_note = genanki.Note(\n",
    "            model=my_model,\n",
    "            fields=[card['topic'], card['information']])\n",
    "        my_deck.add_note(my_note)\n",
    "\n",
    "    # Save the deck to a file\n",
    "    genanki.Package(my_deck).write_to_file(f'{deck_name.replace(\" \", \"_\")}.apkg')\n",
    "    print(f\"Created Anki deck with {len(flashcards)} cards: {deck_name.replace(' ', '_')}.apkg\")\n",
    "    return f'{deck_name.replace(\" \", \"_\")}.apkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e71ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.419295Z",
     "iopub.status.busy": "2025-04-21T04:53:54.418981Z",
     "iopub.status.idle": "2025-04-21T04:53:54.424251Z",
     "shell.execute_reply": "2025-04-21T04:53:54.423542Z"
    },
    "id": "4EH8nx2aRNBq",
    "papermill": {
     "duration": 0.028909,
     "end_time": "2025-04-21T04:53:54.425325",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.396416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def youtube_to_notion_flashcards(url,google_key,notion_key,page_url, num_flashcards,confidence,user_prompt):\n",
    "    configure(api_key=google_key)\n",
    "\n",
    "    # Step 1: Download audio\n",
    "    print(\"Downloading YouTube audio...\")\n",
    "    audio_file = download_youtube_audio(url)\n",
    "\n",
    "    # Step 2: Transcribe\n",
    "    print(\"Transcribing audio...\")\n",
    "    transcript = transcribe_audio(audio_file)\n",
    "\n",
    "    # Step 3: Create embeddings\n",
    "    print(\"Create embeddings...\")\n",
    "    index, chunks, dimension = create_embeddings_and_index(transcript)\n",
    "\n",
    "    # Step 4: Extract key terms\n",
    "    print(\"Extracting key terms...\")\n",
    "    if user_prompt is None or user_prompt == \"\":\n",
    "        user_prompt = \"nouns, technical terms, definitions, important names, or core concepts\"\n",
    "    key_terms, heading = extract_key_terms(transcript,num_flashcards,user_prompt)\n",
    "\n",
    "    # Step 5: Generate flashcards\n",
    "    print(\"Generating flashcards...\")\n",
    "    flashcards = generate_flashcards(key_terms,index,chunks,confidence)\n",
    "\n",
    "    # # Step 6: Upload to Notion\n",
    "    print(\"Uploading to Notion...\")\n",
    "    upload_to_notion(flashcards,notion_key,page_url.split(\"-\")[1],heading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91096379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T04:53:54.475088Z",
     "iopub.status.busy": "2025-04-21T04:53:54.474804Z",
     "iopub.status.idle": "2025-04-21T04:55:32.467180Z",
     "shell.execute_reply": "2025-04-21T04:55:32.466282Z"
    },
    "id": "hm-_gYDFRNBr",
    "outputId": "65103e45-61db-43f1-f65d-0272225eec57",
    "papermill": {
     "duration": 98.018651,
     "end_time": "2025-04-21T04:55:32.468358",
     "exception": false,
     "start_time": "2025-04-21T04:53:54.449707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading YouTube audio...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=aDmp2Uim0zQ&t=115s\n",
      "[youtube] aDmp2Uim0zQ: Downloading webpage\n",
      "[youtube] aDmp2Uim0zQ: Downloading tv client config\n",
      "[youtube] aDmp2Uim0zQ: Downloading player 9a279502-main\n",
      "[youtube] aDmp2Uim0zQ: Downloading tv player API JSON\n",
      "[youtube] aDmp2Uim0zQ: Downloading ios player API JSON\n",
      "[youtube] aDmp2Uim0zQ: Downloading m3u8 information\n",
      "[info] aDmp2Uim0zQ: Downloading 1 format(s): 251\n",
      "[download] Destination: audio.mp3\n",
      "[download] 100% of    5.50MiB in 00:00:00 at 40.37MiB/s  \n",
      "Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 78.4MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create embeddings...\n",
      "Splitting text into chunks...\n",
      "Generating embeddings for 5 chunks...\n",
      "  Embedding chunk 1/5\n",
      "  Embedding chunk 2/5\n",
      "  Embedding chunk 3/5\n",
      "  Embedding chunk 4/5\n",
      "  Embedding chunk 5/5\n",
      "Creating FAISS index with dimension 768...\n",
      "FAISS index created successfully with 5 vectors.\n",
      "Extracting key terms...\n",
      "Extracting key terms...\n",
      "Extracted 10 key terms.\n",
      "Generating flashcards...\n",
      "Generating flashcards for 10 terms...\n",
      "  Processing term 1/10: 'LLM settings'\n",
      "    Successfully generated flashcard for 'LLM settings'.\n",
      "  Processing term 2/10: 'Temperature'\n",
      "    Successfully generated flashcard for 'Temperature'.\n",
      "  Processing term 3/10: 'Top P'\n",
      "    Successfully generated flashcard for 'Top P'.\n",
      "  Processing term 4/10: 'Top K'\n",
      "    Successfully generated flashcard for 'Top K'.\n",
      "  Processing term 5/10: 'Candidate words'\n",
      "    Successfully generated flashcard for 'Candidate words'.\n",
      "  Processing term 6/10: 'Probability scores'\n",
      "    Successfully generated flashcard for 'Probability scores'.\n",
      "  Processing term 7/10: 'Context'\n",
      "    Successfully generated flashcard for 'Context'.\n",
      "  Processing term 8/10: 'Sampling'\n",
      "    Successfully generated flashcard for 'Sampling'.\n",
      "  Processing term 9/10: 'Deterministic output'\n",
      "    Skipping card for 'Deterministic output' due to low confidence (0.3).\n",
      "  Processing term 10/10: 'Creative output'\n",
      "    Error processing term 'Creative output': 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "Generated 8 high-confidence flashcards.\n",
      "Uploading to Notion...\n",
      "Uploading flashcard: 1/8\n",
      "Uploading flashcard: 2/8\n",
      "Uploading flashcard: 3/8\n",
      "Uploading flashcard: 4/8\n",
      "Uploading flashcard: 5/8\n",
      "Uploading flashcard: 6/8\n",
      "Uploading flashcard: 7/8\n",
      "Uploading flashcard: 8/8\n",
      "Created a new Notion database and uploaded 8 cards. You can access them here: https://www.notion.so/1dc33276f2848140949dc69f7cb0c52e\n"
     ]
    }
   ],
   "source": [
    "### Static Inputs\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "google_key = user_secrets.get_secret(\"API_KEY\")                    # Add your Google key from Google AI Studio\n",
    "notion_key = \"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\"   # Add your key from notion. Can be created at notion's creator profile. Connection has to be made within the page and creator profile which is very simple process. A quick read into notions API Integration documentation should give more insights. Link - https://developers.notion.com/\n",
    "\n",
    "# User Inputs\n",
    "page_url = \"https://www.notion.so/flashcards-1ce33276f2848062aec9ddb3763230f6\"     # Add the notion page url where you want to store the flashcard.\n",
    "url = \"https://www.youtube.com/watch?v=aDmp2Uim0zQ&t=115s\"                         # Give url for youtube video\n",
    "num_flashcards = 10                                                                 # Input the number of flashcards required\n",
    "confidence = 0.85                                                                   # Add the confidence coefficient to form accurate flashcards.\n",
    "user_prompt = \"\"\n",
    "\n",
    "youtube_to_notion_flashcards(url,google_key,notion_key,page_url,num_flashcards,confidence,user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9198f",
   "metadata": {
    "id": "XumupuSbE7AD",
    "papermill": {
     "duration": 0.025293,
     "end_time": "2025-04-21T04:55:32.519536",
     "exception": false,
     "start_time": "2025-04-21T04:55:32.494243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 201.963781,
   "end_time": "2025-04-21T04:55:35.354913",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T04:52:13.391132",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
